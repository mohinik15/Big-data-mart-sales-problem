{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d51eedb9",
   "metadata": {},
   "source": [
    "# Big Data Mart Sales Problem Project"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a6bec45",
   "metadata": {},
   "source": [
    "The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the sales of each product at a particular store.\n",
    "\n",
    "Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing the sales of their products.\n",
    "\n",
    "The dataset includes two files:\n",
    "\n",
    "bigdatamart_Train.csv: Use this file for the model building purpose.\n",
    "\n",
    "bigdatamart_Test.csv: Use this file for getting predictions from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfe64ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f90f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = pd.read_csv(\"https://raw.githubusercontent.com/dsrscientist/bigdatamart_rep/master/bigdatamart_Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eeb84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset_original = pd.read_csv(\"https://raw.githubusercontent.com/dsrscientist/bigdatamart_rep/master/bigdatamart_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988b9833",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704fbb01",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e77658",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f222f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260b7a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdff2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c78129",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf99d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_columns = []\n",
    "for x in training_dataset.dtypes.index:\n",
    "    if training_dataset.dtypes[x] == 'object':\n",
    "        obj_columns.append(x)\n",
    "        \n",
    "\n",
    "obj_columns.remove('Item_Identifier')\n",
    "obj_columns.remove('Outlet_Identifier')\n",
    "\n",
    "obj_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe8cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in obj_columns:\n",
    "    print(col)\n",
    "    print(training_dataset[col].value_counts())\n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b950c9",
   "metadata": {},
   "source": [
    "# Filling missing values present in our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_weight_mean = training_dataset.pivot_table(values = \"Item_Weight\", index = 'Item_Identifier')\n",
    "item_weight_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ff598",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = training_dataset['Item_Weight'].isnull()\n",
    "\n",
    "for i, item in enumerate(training_dataset['Item_Identifier']):\n",
    "    if missing_data[i]:\n",
    "        if item in item_weight_mean:\n",
    "            training_dataset['Item_Weight'][i] = item_weight_mean.loc[item]['Item_Weight']\n",
    "        else:\n",
    "            training_dataset['Item_Weight'][i] = np.mean(training_dataset['Item_Weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ebf610",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset['Item_Weight'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea029f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset['Item_Weight'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc83854",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_val = training_dataset['Outlet_Size'].isnull()\n",
    "training_dataset.loc[missing_val, 'Outlet_Size'] = training_dataset.loc[missing_val, 'Outlet_Type'].apply(lambda x: outlet_size_mode[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8629eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset['Outlet_Size'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692bda2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(training_dataset['Item_Visibility']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd346e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset.loc[:, 'Item_Visibility'].replace([0], [training_dataset['Item_Visibility'].mean()], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd849a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(training_dataset['Item_Visibility']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1549dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset['Item_Fat_Content'] = training_dataset['Item_Fat_Content'].replace({'LF':'Low Fat', 'reg':'Regular', 'low fat':'Low Fat'})\n",
    "training_dataset['Item_Fat_Content'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727016d9",
   "metadata": {},
   "source": [
    "# Adding more columns/categories from the existing one's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae6fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset['New_Item_Type'] = training_dataset['Item_Identifier'].apply(lambda x: x[:2])\n",
    "training_dataset['New_Item_Type'] = training_dataset['New_Item_Type'].map({'FD':'Food', 'NC':'Non-Consumable', 'DR':'Drinks'})\n",
    "training_dataset['New_Item_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7373bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset.loc[training_dataset['New_Item_Type']=='Non-Consumable', 'Item_Fat_Content'] = 'Non-Edible'\n",
    "training_dataset['Item_Fat_Content'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc16a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset['Outlet_Years'] = 2013 - training_dataset['Outlet_Establishment_Year']\n",
    "training_dataset['Outlet_Years'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569bc6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = training_dataset.drop(\"Outlet_Establishment_Year\", axis=1)\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538799c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02efa6d",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa18a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "values = list(training_dataset['Item_Type'].unique())\n",
    "diag = sns.countplot(training_dataset[\"Item_Type\"])\n",
    "diag.set_xticklabels(labels=values, rotation=90)\n",
    "plt.title(\"Item Type Column Details\\n\")\n",
    "plt.xlabel(\"Product category names\")\n",
    "plt.ylabel(\"Count of rows in the dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb8051",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "sns.countplot(training_dataset[\"Item_Fat_Content\"])\n",
    "plt.ylabel(\"Count of rows in the dataset\")\n",
    "plt.xlabel(\"Item Categories with respect to Fat\")\n",
    "plt.title(\"Item_Fat_Content Column Details\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "sns.countplot(training_dataset[\"Outlet_Size\"])\n",
    "plt.ylabel(\"Count of rows in the dataset\")\n",
    "plt.xlabel(\"Outlet Size Variations\")\n",
    "plt.title(\"Outlet_Size Column Details\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b741e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "sns.countplot(training_dataset[\"Outlet_Location_Type\"])\n",
    "plt.ylabel(\"Count of rows in the dataset\")\n",
    "plt.xlabel(\"Outlet Location on Tier Level\")\n",
    "plt.title(\"Outlet_Location_Type Column Details\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a62907",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(training_dataset[\"Outlet_Type\"])\n",
    "plt.ylabel(\"Count of rows in the dataset\")\n",
    "plt.xlabel(\"Outlet Type based on it's interiors\")\n",
    "plt.title(\"Outlet_Type Column Details\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1f5e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b26cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, nrows=1, figsize=(12,7))\n",
    "index = 0\n",
    "ax = ax.flatten()\n",
    "numeric_column_names = [\"Item_Weight\", \"Item_Visibility\", \"Item_MRP\", \"Item_Outlet_Sales\"]\n",
    "for col, value in training_dataset[numeric_column_names].items():\n",
    "    sns.boxplot(y=col, data=training_dataset, ax=ax[index], palette=\"bone\")\n",
    "    index += 1\n",
    "plt.tight_layout(pad=0.2, w_pad=0.9, h_pad=5.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab606139",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, nrows=1, figsize=(12,7))\n",
    "index = 0\n",
    "ax = ax.flatten()\n",
    "numeric_column_names = [\"Item_Weight\", \"Item_Visibility\", \"Item_MRP\", \"Item_Outlet_Sales\"]\n",
    "for col, value in training_dataset[numeric_column_names].items():\n",
    "    sns.distplot(value, ax=ax[index], hist=False, color=\"r\", kde_kws={\"shade\": True})\n",
    "    index += 1\n",
    "plt.tight_layout(pad=0.5, w_pad=0.7, h_pad=5.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807dc19f",
   "metadata": {},
   "source": [
    "# Correlation using a Heatmap"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2cf42353",
   "metadata": {},
   "source": [
    "Positive correlation - A correlation of +1 indicates a perfect positive correlation, meaning that both variables move in the same direction together.\n",
    "\n",
    "Negative correlation - A correlation of â€“1 indicates a perfect negative correlation, meaning that as one variable goes up, the other goes down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cf245",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_triangle = np.tril(training_dataset.corr())\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(training_dataset.corr(), vmin=-1, vmax=1, annot=True, square=True, fmt='0.5f', \n",
    "            annot_kws={'size':10}, cmap=\"Set3\", mask=lower_triangle)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f126de46",
   "metadata": {},
   "source": [
    "# Log Transformation on the Target Label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d40579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transformation on \"Item_Outlet_Sales\" column\n",
    "\n",
    "training_dataset['Item_Outlet_Sales'] = np.log(1+training_dataset['Item_Outlet_Sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8aca8e",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b644e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = training_dataset.drop([\"Item_Identifier\",\"Outlet_Identifier\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ed57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "col_name = [\"Item_Type\"]\n",
    "for col in col_name:\n",
    "    training_dataset[col] = label_encoder.fit_transform(training_dataset[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d442ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = pd.get_dummies(training_dataset, columns=['Item_Fat_Content', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'New_Item_Type'])\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06b7d2",
   "metadata": {},
   "source": [
    "# Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fe955",
   "metadata": {},
   "outputs": [],
   "source": [
    "triangle = np.triu(training_dataset.corr())\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.heatmap(training_dataset.corr(), vmin=-1, vmax=1, annot=True, square=True, fmt='0.3f', \n",
    "            annot_kws={'size':10}, cmap=\"YlGnBu\", mask=triangle)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d839b2a",
   "metadata": {},
   "source": [
    "# Splitting the dataset into 2 variables namely 'X' and 'Y' for feature and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a46b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_dataset.drop('Item_Outlet_Sales', axis=1)\n",
    "Y = training_dataset['Item_Outlet_Sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e106ef",
   "metadata": {},
   "source": [
    "# Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "X # Displaying all the features after applying scaling technique to avoid bias output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08743acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=111)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84241f65",
   "metadata": {},
   "source": [
    "# Machine Learning Model for Regression and Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b81547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Model Function\n",
    "\n",
    "def reg(model, X, Y):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=111)\n",
    "    \n",
    "    # Training the model\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # Predicting Y_test\n",
    "    pred = model.predict(X_test)\n",
    "    \n",
    "    # RMSE - a lower RMSE score is better than a higher one\n",
    "    rmse = mean_squared_error(Y_test, pred, squared=False)\n",
    "    print(\"RMSE Score is:\", rmse)\n",
    "    \n",
    "    # R2 score\n",
    "    r2 = r2_score(Y_test, pred, multioutput='variance_weighted')*100\n",
    "    print(\"R2 Score is:\", r2)\n",
    "    \n",
    "    # Cross Validation Score\n",
    "    cv_score = (cross_val_score(model, X, Y, cv=5).mean())*100\n",
    "    print(\"Cross Validation Score:\", cv_score)\n",
    "    \n",
    "    # Result of r2 score minus cv score\n",
    "    result = r2 - cv_score\n",
    "    print(\"R2 Score - Cross Validation Score is\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e150014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "\n",
    "model=LinearRegression()\n",
    "reg(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4482f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Regression\n",
    "\n",
    "model=SVR(C=1.0, epsilon=0.2, kernel='poly', gamma='auto')\n",
    "reg(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c98220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regressor\n",
    "\n",
    "model=DecisionTreeRegressor(criterion=\"poisson\", random_state=111)\n",
    "reg(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b215e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor\n",
    "\n",
    "model=RandomForestRegressor(max_depth=2, max_features=\"sqrt\")\n",
    "reg(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b9e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Neighbors Regressor\n",
    "\n",
    "KNeighborsRegressor(n_neighbors=2, algorithm='kd_tree')\n",
    "reg(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c596de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Regressor\n",
    "\n",
    "model=GradientBoostingRegressor(loss='quantile', n_estimators=200, max_depth=5)\n",
    "reg(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a8fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ada Boost Regressor\n",
    "\n",
    "model=AdaBoostRegressor(n_estimators=300, learning_rate=1.05, random_state=42)\n",
    "reg(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c037b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees Regressor\n",
    "\n",
    "model=ExtraTreesRegressor(n_estimators=200, max_features='sqrt', n_jobs=6)\n",
    "reg(model, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3e6",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32414526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing Support Vector Regression\n",
    "\n",
    "fmod_param = {'kernel' : [\"linear\", \"rbf\"],\n",
    "              'gamma' : [\"scale\", \"auto\"],\n",
    "              'C' : [2.0, 4.0],\n",
    "              'epsilon' : [0.2, 0.4]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3722ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "GSCV = GridSearchCV(SVR(), fmod_param, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d181f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GSCV.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d71a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "GSCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Model = SVR(C=2.0, epsilon=0.4, gamma=\"auto\", kernel=\"rbf\")\n",
    "Model_Training = Final_Model.fit(X_train, Y_train)\n",
    "fmod_pred = Final_Model.predict(X_test)\n",
    "fmod_r2 = r2_score(Y_test, fmod_pred, multioutput='variance_weighted')*100\n",
    "print(\"R2 score for the Best Model is:\", fmod_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93386e0",
   "metadata": {},
   "source": [
    "# Pre processing the Testing Dataset to predict the Sales column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd017692",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = testing_dataset_original.copy()\n",
    "testing_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3fad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling missing data in the testing dataset for column \"Item_Weight\"\n",
    "\n",
    "item_weight_mean = testing_dataset.pivot_table(values = \"Item_Weight\", index = 'Item_Identifier')\n",
    "missing_data = testing_dataset['Item_Weight'].isnull()\n",
    "for i, item in enumerate(testing_dataset['Item_Identifier']):\n",
    "    if missing_data[i]:\n",
    "        if item in item_weight_mean:\n",
    "            testing_dataset['Item_Weight'][i] = item_weight_mean.loc[item]['Item_Weight']\n",
    "        else:\n",
    "            testing_dataset['Item_Weight'][i] = np.mean(testing_dataset['Item_Weight'])\n",
    "            \n",
    "# filling missing data in the testing dataset for column \"Outlet_Size\"\n",
    "\n",
    "outlet_size_mode = testing_dataset.pivot_table(values='Outlet_Size', columns='Outlet_Type', aggfunc=(lambda x: x.mode()[0]))\n",
    "missing_val = testing_dataset['Outlet_Size'].isnull()\n",
    "testing_dataset.loc[missing_val, 'Outlet_Size'] = testing_dataset.loc[missing_val, 'Outlet_Type'].apply(lambda x: outlet_size_mode[x])\n",
    "\n",
    "# filling zero values in the testing dataset for column \"Item_Visibility\"\n",
    "\n",
    "testing_dataset.loc[:, 'Item_Visibility'].replace([0], [testing_dataset['Item_Visibility'].mean()], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aaf510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clubbing similar data rows together for column \"Item_Fat_Content\" using replace\n",
    "\n",
    "testing_dataset['Item_Fat_Content'] = testing_dataset['Item_Fat_Content'].replace({'LF':'Low Fat', 'reg':'Regular', 'low fat':'Low Fat'})\n",
    "testing_dataset['Item_Fat_Content'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1a22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column named \"New_Item_Type\" and adding proper categories using map\n",
    "\n",
    "testing_dataset['New_Item_Type'] = testing_dataset['Item_Identifier'].apply(lambda x: x[:2])\n",
    "testing_dataset['New_Item_Type'] = testing_dataset['New_Item_Type'].map({'FD':'Food', 'NC':'Non-Consumable', 'DR':'Drinks'})\n",
    "testing_dataset['New_Item_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ec041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column named \"Item_Fat_Content\" and adding proper categories\n",
    "\n",
    "testing_dataset.loc[testing_dataset['New_Item_Type']=='Non-Consumable', 'Item_Fat_Content'] = 'Non-Edible'\n",
    "testing_dataset['Item_Fat_Content'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e723613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column named \"Outlet_Years\" and removing the column \"Outlet_Establishment_Year\" that was used to derive it\n",
    "\n",
    "testing_dataset['Outlet_Years'] = 2013 - testing_dataset['Outlet_Establishment_Year']\n",
    "testing_dataset = testing_dataset.drop(\"Outlet_Establishment_Year\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e3a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the ID columns \"Item_Identifier\" and \"Outlet_Identifier\"\n",
    "\n",
    "testing_dataset = testing_dataset.drop([\"Item_Identifier\",\"Outlet_Identifier\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3652db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "col_name = [\"Item_Type\"]\n",
    "for col in col_name:\n",
    "    testing_dataset[col] = label_encoder.fit_transform(testing_dataset[col])\n",
    "\n",
    "# One Hot Encoder\n",
    "testing_dataset = pd.get_dummies(testing_dataset, columns=['Item_Fat_Content', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'New_Item_Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling on all testing dataset rows\n",
    "\n",
    "scaler = StandardScaler()\n",
    "testing_dataset = pd.DataFrame(scaler.fit_transform(testing_dataset), columns=testing_dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1e2b8d",
   "metadata": {},
   "source": [
    "# Predition result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3631bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Item_Outlet_Sales from the feature columns of our Testing dataset\n",
    "\n",
    "Predicted_Sales = Final_Model.predict(testing_dataset)\n",
    "\n",
    "# Reversing the Log Transformation that was performed on the Target column while training the ML Model\n",
    "\n",
    "Predicted_Sales = np.exp(Predicted_Sales)\n",
    "Predicted_Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e971f6c",
   "metadata": {},
   "source": [
    "# Converting the sales output back in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05887811",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output = pd.DataFrame()\n",
    "predicted_output['Item_Outlet_Sales']=Predicted_Sales\n",
    "predicted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b37cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output.to_csv(\"Predicted_Sales_Data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
